name: auditing-content
slug: auditing-content
version: 1.0.0
description: Analyze pages for LLM trust signals and retrieval resilience. Scores pages across 5 trust layers to identify content that may be misrepresented by AI systems.

# Trigger patterns - when this skill should activate
triggers:
  - "/audit-llm"
  - "audit this page for AI readability"
  - "check if LLMs will misquote this"
  - "evaluate content for AI retrieval"
  - "score this page for trust signals"
  - "find claims that AI might misrepresent"
  - "analyze for AI safety"

# Natural language examples for intent matching
examples:
  - context: "User concerned about brand misrepresentation"
    user_says: "Will AI systems quote our security page correctly?"
    agent_action: "Launch auditing-content to analyze security page for trust signals and chunk vulnerability"
  - context: "User reviewing marketing content"
    user_says: "Our pricing page has a lot of claims without dates. Will that be a problem for LLMs?"
    agent_action: "Launch auditing-content to audit pricing page for temporal markers and claim verifiability"
  - context: "User preparing content for publication"
    user_says: "Before we publish this docs page, can you check if it's AI-readable?"
    agent_action: "Launch auditing-content to score documentation page across 5 trust layers"

# Domain hints for proactive suggestion
domain_hints:
  - llm-trust
  - ai-readability
  - content-audit
  - retrieval
  - trust-signals
  - claim-verification
  - brand-safety
  - ai-resilience

entry: SKILL.md

capabilities:
  model_tier: sonnet
  danger_level: safe
  effort_hint: medium
  downgrade_allowed: true
  execution_hint: sequential
  requires:
    native_runtime: false
    tool_calling: true
    thinking_traces: false
    vision: false

allowed-tools:
  - Read
  - Write
  - Glob
  - Grep
